<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://vocdex.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://vocdex.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-10-08T18:04:01+00:00</updated><id>https://vocdex.github.io/feed.xml</id><title type="html">blank</title><subtitle>My website </subtitle><entry><title type="html">Exploring Model-Based RL</title><link href="https://vocdex.github.io/blog/2023/mbrl/" rel="alternate" type="text/html" title="Exploring Model-Based RL"/><published>2023-11-01T00:00:00+00:00</published><updated>2023-11-01T00:00:00+00:00</updated><id>https://vocdex.github.io/blog/2023/mbrl</id><content type="html" xml:base="https://vocdex.github.io/blog/2023/mbrl/"><![CDATA[<p>Reinforcement Learning (RL) aims to derive an optimal policy for a Markov Decision Process (MDP). When we possess a perfect world model, achieving this optimal policy is straightforward. However, in most real-world scenarios, such a model is unknown and must be learned from data or manually designed.</p> <h2 id="model-free-rl-vs-model-based-rl">Model-Free RL vs Model-Based RL</h2> <p>In Model-Free RL, we focus on deriving a controller without explicitly learning a model of the environment. Utilizing a replay buffer, which essentially acts as a <strong>non-parametric world model</strong> containing state-action transitions, we estimate the value function directly. In contrast, Model-Based RL strives to learn the underlying MDP by estimating transition probabilities \(P(s' | s, a)\) and the reward function \(R(s, a)\). This acquired <strong>parametric world model</strong> allows us to derive optimal policies through planning methods.</p> <p>The debate regarding the superior approach—model-free or model-based—is ongoing. Model-free methods often demand extensive interaction with the environment, making them costly for real-world training. Consequently, these methods are primarily applied in simulated environments, with a few exceptions.</p> <p>On the other hand, model-based reinforcement learning holds promise for superior sample efficiency (the amount of experience that an agent needs to generate in an environment during training in order to reach a certain level of performance). Yet, accurately learning a generalized model of the environment poses a significant challenge. The compounding error problem and the potential exploitation of learned model deficiencies are some of the challenges.</p> <h1 id="what-is-a-world-model">What is a (world) model?</h1> <p>Before getting into the details, let’s clarify what we mean by “world models” to avoid any <a href="https://twitter.com/DrJimFan/status/1709947595525951787">potential confusion</a> .</p> <p>An internal world model serves as a mechanism to represent both the system and its environment. For instance, a robot equipped with an internal model possesses knowledge about the underlying physics of the world and can generate and test “what-if” hypotheses, such as the consequences of different actions:</p> <ul> <li>What if I carry out action \(x\) ? and . . .</li> <li>Where would I be if I turned right or left here?</li> </ul> <p>Holland et al. writes:</p> <blockquote> <p>an internal model allows a system to look ahead to the future consequences of current actions, <strong>without</strong> actually committing itself to those actions”.</p> </blockquote> <p>Yann LeCun highlights the necessity of world models for minimizing costly and dangerous <a href="https://openreview.net/pdf?id=BZ5a1r-kVsf">real-world interactions</a></p> <blockquote> <p>Interactions in the real world are expensive and dangerous, intelligent agents should learn as much as they can about the world without interaction (by observation) so as to minimize the number of expensive and dangerous trials necessary to learn a particular task.</p> </blockquote> <p>In simple mathematical terms, world models learns a function \(F: S_{t} \times A→ S_{t+1}\) or predicting the next state given the observed state(s) and current action.</p> <h1 id="general-algorithm">General Algorithm</h1> <p>This picture illustrates the general paradigm of model-based RL:</p> <p><img src="https://github.com/vocdex/vocdex.github.io/assets/19290583/299bb5f7-3849-4f39-864e-9c230547312d" alt="mbrl_general"/></p> <ul> <li>Start with some initial policy \(\pi\) and no initial \(D\) replay buffer</li> </ul> <p>Iterate for several episodes:</p> <ul> <li>Learn a model from collected data \(D\)</li> <li>Plan a new policy \(\pi\) based on the estimated models in <strong>imagination</strong></li> <li>Roll out policy \(\pi\) to collect additional data</li> </ul> <p>We have three questions to answer to accomplish this:</p> <ol> <li>How do we learn model?</li> <li>How do we plan given a model?</li> <li>How to we trade exploration and exploitation?</li> </ol> <h1 id="model-learning">Model Learning</h1> <p>We need to learn a model that approximates the unknown MDP. We focus on the fully observable case (state \(x_t\) is observed).</p> <h3 id="data-in-rl">“Data” in RL</h3> <p>Data consists of trajectories \((x_0, a_0, r_0, x_1, a_1, r_1, …)\)</p> <p>Common settings:</p> <ul> <li>Episodic settings: agent learns over multiple “training” episodes, each resulting in a new trajectory, after which the environment “resets”</li> <li>Non-episodic/continous setting: agent learns “online”, yielding a single trajectory</li> </ul> <p><strong>Key Insight:</strong></p> <p>\(x_{t+1}\) is conditionally independent of \(x_{1:t-1}\) given \(x_t\), \(a_t\)</p> <p>In fully observed environments, due to conditional independence, learning \(F\) and \(R\) is essentially a <strong>regression (density estimation)</strong> problem (supervised learning)</p> <h3 id="delta-state-prediction">Delta state prediction</h3> <p>This allows the model to focus on capturing the changes accurately.</p> <h3 id="key-pitfalls-in-mbrl">Key pitfalls in MBRL</h3> <p>Errors in the model estimate compound when planning over multiple time-steps.</p> <p>This compounding error is exploited by planning algorithm (MPC, policy search)</p> <h3 id="epistemic-uncertainty">Epistemic uncertainty</h3> <h3 id="aleatoric-uncertainty">Aleatoric uncertainty</h3> <h3 id="pets-algorithm">PETS Algorithm</h3> <h3 id="learning-latent-state-representations">Learning latent state representations</h3> <p>World Models, PlaNet (Hafner, ICML 19), Dreamer (Hafner, ICLR 20), MuZero(Schrittwieser, Nature 20), IRIS( Micheli + ICLR 23)</p> <p>Dreamer v1, v2, DayDreamer ( application of Dreamer in 5 different real robots)</p> <h1 id="planning">Planning</h1> <p>To start, assume we have a known deterministic model for the reward and dynamics</p> \[x_{t+1} = f (x_t, a_t)\] <p>Then, our objective becomes to plan sequences of actions such that over a fixed horizon, our chosen actions will yield the maximum reward. Notice, here we cannot explicitly optimize over an infinite horizon due to model errors and noise.</p> <p><strong>Key idea:</strong> plan over a finite horizon h, carry out first action, then replan ( inside world model):</p> <ul> <li>at each iteration \(t\), observe \(s_t\)</li> <li>optimize performance over horizon h (maximize cumulative reward of actions chosen by the planner)</li> <li>carry out action \(a_t\), then replan</li> </ul> <p>How to optimize for action sequences with the maximum cumulative reward during h-steps?</p> <p>Answer:</p> <ul> <li>For continuous actions,differentiable rewards, and differentiably dynamics, we can analytically compute gradients (backpropagation through time)</li> </ul> <p>Challenges(especially for large \(h\)): local minima, vanishing/exploding gradients. Therefore, in practice, this optimization is done with a variety of gradient-free sample-based optimizers below.</p> <h2 id="online-planning-for-closed-loop-control">Online Planning for closed-loop control</h2> <p>We use MPC to select actions via our model predictions in imagination. At each time step, the method perfroms a short-horizon trajectory optimization, using the model to predict the outcomes of different action sequences.</p> <h2 id="random-shooting">Random Shooting:</h2> <ul> <li>Randomly sample multiple action sequences</li> <li>Simulate each action sequence to predict the resulting trajectory</li> <li>Evaluate each trajectory using a cost or objective function</li> <li>Selecting the action sequence associated with the best trajectory.</li> </ul> <p>In short, random shooting is a direct, brute-force method for trajectory optimization. It can be a good choice where the action space isn’t too vast, or computational resources are abundant, making it inapplicable for robotic manipulation tasks. <a href="A. Nagabandi, G. Kahn, R. S. Fearing, and S. Levine. Neural network dynamics for modelbased deep reinforcement learning with model-free fine-tuning. I">Nagabandi et al(2017)</a> has first used Random Shooting method in continuous control tasks with learned models. Since Random Shooting has number of drawbacks due to scalability with dimensions, a lot of the recent literature focused on integrating a better planing algorithm called Cross-Entropy Method or its improved variants.</p> <h2 id="cross-entropy-method-cem">Cross-entropy method (CEM)</h2> <p>A smarter way to do random shooting is called cross-entropy method <a href="https://people.smp.uq.edu.au/DirkKroese/ps/CEopt.pdf">CEM</a>. Highly recommended to refer to <a href="http://web.mit.edu/6.454/www/www_fall_2003/gew/CEtutorial.pdf">A Tutorial on Cross-Entropy Method</a></p> <p>It begins as a random shooting approach, but then does this sampling for multiple iterations \(m [0, M]\) at each time step. The top \(J\) highest scoring actions sequences (elites) from each iteration are used to update mean and variance of the sampling distribution for the next iteration.</p> <p>In simpler terms, you try out random stuff at the beginning and you evaluate this random actions based on some criteria (how close you are to the target or how much reward you receive). You choose \(J\) number of highest scoring elites from this sequences and in the next iteration of trials, you will try to sample closer to those elites. You’ll keep doing this.</p> <h2 id="random-shooting-vs-cem">Random Shooting vs CEM</h2> <ul> <li>While both Random Shooting and CEM are sampling-based methods, CEM iteratively refines its action samples. After each iteration in CEM, the action distribution is updated to focus more on the promising regions of the action space (based on the elite samples). In contrast, Random Shooting does not have this iterative refinement; it samples randomly every time.</li> <li>As a result, CEM can potentially find better solutions with fewer samples than Random Shooting because it focuses on promising regions. However, the simplicity of Random Shooting might be favorable in some situations.</li> </ul> <h2 id="icem">iCEM</h2> <p>In CEM, action plan has no temporal correlation and it doesn’t have a memory of the actions that led to better behaviours in the past iterations.</p> <p><a href="https://martius-lab.github.io/iCEM/">iCEM</a> improves over CEM by adding simple but smart tricks:</p> <ul> <li>Colored noise: ensures temporal correlation between action plans</li> <li>Memory</li> <li>Population decay</li> </ul> <p align="center"> <img width="350" alt="iCEM screenshot" src="https://github.com/vocdex/vocdex.github.io/assets/19290583/22d6eb19-c1aa-43d3-9fc4-3142c50705c9"/> </p> <p>The figure is from Cristina Pinneri’s <a href="https://www.youtube.com/watch?v=wuOOIuC1_h4">presentation</a></p> <p>The result: \(2.7-22 \times\) more sample efficient and faster \(1.2-10 \times\) faster than CEM.</p> <h3 id="planning-horizon-dilemma">Planning Horizon Dilemma</h3> <p>From <a href="https://arxiv.org/pdf/1907.02057.pdf">Benchmarking Model-Based Reinforcement Learning</a></p> <h1 id="unsupervised-exploration-in-mbrl">Unsupervised Exploration in MBRL</h1>]]></content><author><name>Shukrullo Nazirjonov</name></author><summary type="html"><![CDATA[Minimal overview of model-based RL methods(updated regularly)]]></summary></entry></feed>